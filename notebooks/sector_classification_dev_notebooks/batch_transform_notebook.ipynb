{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Transform notebook\n",
    "- Notebook initiates SageMaker Batch Transform job for a HuggingFace model stored in S3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate input file for batch transform\n",
    "- Inputs csv file and outputs jsonl file\n",
    "- Adds \"return_all_scores\" parameter to file so that batch transform job will return all inference scores per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputs': 'Theta Lake is a machine learning based cyber security company with a focus on modern work collaboration tools.', 'parameters': {'return_all_scores': True, 'truncation': True, 'max_length': 512}}\n",
      "{'inputs': 'Pluribus Networks is the developer of an open, virtualized and highly programmable network fabric for next generation Data Centers with simplified management and white-box economics.', 'parameters': {'return_all_scores': True, 'truncation': True, 'max_length': 512}}\n",
      "{'inputs': 'Remine is the developer of a home buying platform designed to efficiently connect mortgage lenders, real estate agents, and consumers in one streamlined experience.', 'parameters': {'return_all_scores': True, 'truncation': True, 'max_length': 512}}\n",
      "{'inputs': 'Minted is a lifestyle brand and developer of a design marketplace designed to connect users with the world’s best artists to create something one of a kind.', 'parameters': {'return_all_scores': True, 'truncation': True, 'max_length': 512}}\n",
      "{'inputs': \"EcoVadis, founded in 2007, is one of the world's most trusted providers of business sustainability ratings. Global supply chains, financial institutions, and public organizations rely on EcoVadis to monitor and improve the sustainability performance of their business and trading partners. Backed by a powerful technology platform, EcoVadis’ evidence-based ratings are validated by a global team of experts, and are adapted to more than 200 industry categories, 160 countries, and companies of all sizes.\", 'parameters': {'return_all_scores': True, 'truncation': True, 'max_length': 512}}\n",
      "outputs/data.jsonl uploaded to s3://sector-classification-aiml/batch_transform/input/data.jsonl\n"
     ]
    }
   ],
   "source": [
    "# generate input file\n",
    "import csv\n",
    "import json\n",
    "from sagemaker.s3 import S3Uploader, s3_path_join\n",
    "\n",
    "# datset files\n",
    "dataset_csv_file = \"sample_data/sample_data_input.csv\"\n",
    "dataset_jsonl_file = \"outputs/data.jsonl\"\n",
    "\n",
    "with open(dataset_csv_file, \"r+\") as infile, open(dataset_jsonl_file, \"w+\") as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    for row in reader:\n",
    "        row[\"parameters\"] = {\"return_all_scores\": True, \"truncation\":True, \"max_length\":512}\n",
    "        json.dump(row, outfile)\n",
    "        print(row)\n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "sagemaker_session_bucket = \"sector-classification-aiml\"\n",
    "# uploads a given file to S3.\n",
    "input_s3_path = s3_path_join(\"s3://\", sagemaker_session_bucket, \"batch_transform/input\")\n",
    "output_s3_path = s3_path_join(\n",
    "    \"s3://\", sagemaker_session_bucket, \"batch_transform/output\"\n",
    ")\n",
    "s3_file_uri = S3Uploader.upload(dataset_jsonl_file, input_s3_path)\n",
    "\n",
    "print(f\"{dataset_jsonl_file} uploaded to {s3_file_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model_uri = os.environ.get(\"MODEL_URI\") # S3 URI path to trained model\n",
    "role = os.environ.get(\"SAGEMAKER_ROLE\")\n",
    "\n",
    "transformers_version = \"4.26\"\n",
    "pytorch_version = \"1.13\"\n",
    "python_version = \"py39\"\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=model_uri,\n",
    "    role=role,\n",
    "    transformers_version=transformers_version,\n",
    "    pytorch_version=pytorch_version,\n",
    "    py_version=python_version,\n",
    "    env={\"HF_TASK\": \"text-classification\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch transform job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: huggingface-pytorch-inference-2024-06-11-16-03-33-799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................\n",
      "Warning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2024-06-11T16:09:48,201 [INFO ] main com.amazonaws.ml.mms.ModelServer - \n",
      "MMS Home: /opt/conda/lib/python3.9/site-packages\n",
      "Current directory: /\n",
      "Temp directory: /home/model-server/tmp\n",
      "Number of GPUs: 0\n",
      "Number of CPUs: 4\n",
      "Max heap size: 4008 M\n",
      "Python executable: /opt/conda/bin/python3.9\n",
      "Config file: /etc/sagemaker-mms.properties\n",
      "Inference address: http://0.0.0.0:8080\n",
      "Management address: http://0.0.0.0:8080\n",
      "Model Store: /\n",
      "Initial Models: model=/opt/ml/model\n",
      "Log dir: null\n",
      "Metrics dir: null\n",
      "Netty threads: 0\n",
      "Netty client threads: 0\n",
      "Default workers per model: 4\n",
      "Blacklist Regex: N/A\n",
      "Maximum Response Size: 6553500\n",
      "Maximum Request Size: 6553500\n",
      "Preload model: false\n",
      "Prefer direct buffer: false\n",
      "2024-06-11T16:09:48,212 [INFO ] main com.amazonaws.ml.mms.ModelServer - Loading initial models: /opt/ml/model preload_model: false\n",
      "2024-06-11T16:09:48,288 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-model\n",
      "2024-06-11T16:09:48,394 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_huggingface_inference_toolkit.handler_service --model-path /opt/ml/model --model-name model --preload-model false --tmp-dir /home/model-server/tmp\n",
      "2024-06-11T16:09:48,397 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\n",
      "2024-06-11T16:09:48,398 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 42\n",
      "2024-06-11T16:09:48,398 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\n",
      "2024-06-11T16:09:48,398 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.9.13\n",
      "2024-06-11T16:09:48,400 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\n",
      "2024-06-11T16:09:48,411 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "2024-06-11T16:09:48,434 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "2024-06-11T16:09:48,434 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "2024-06-11T16:09:48,437 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "2024-06-11T16:09:48,438 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "2024-06-11T16:09:48,559 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "Model server started.\n",
      "2024-06-11T16:09:48,565 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "2024-06-11T16:09:48,570 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "2024-06-11T16:09:48,571 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\n",
      "2024-06-11T16:09:48,573 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "2024-06-11T16:09:48,573 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "2024-06-11T16:09:54,116 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000012-00000000-b684728f02100ea0-ed33286e\n",
      "2024-06-11T16:09:54,141 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 5502\n",
      "2024-06-11T16:09:54,144 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-3\n",
      "2024-06-11T16:09:54,345 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000012-00000001-c9c1728f02100ea0-534e630c\n",
      "2024-06-11T16:09:54,347 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 5707\n",
      "2024-06-11T16:09:54,347 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-2\n",
      "2024-06-11T16:09:54,404 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000012-00000004-ed36be8f02100ea3-4c476316\n",
      "2024-06-11T16:09:54,407 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 5739\n",
      "2024-06-11T16:09:54,407 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-4\n",
      "2024-06-11T16:09:54,567 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000012-00000002-bc1df28f02100ea0-0a433dbd\n",
      "2024-06-11T16:09:54,568 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 5928\n",
      "2024-06-11T16:09:54,569 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-1\n",
      "2024-06-11T16:09:56,191 [INFO ] pool-2-thread-6 ACCESS_LOG - /169.254.255.130:47688 \"GET /ping HTTP/1.1\" 200 22\n",
      "2024-06-11T16:09:56,225 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:47700 \"GET /execution-parameters HTTP/1.1\" 404 1\n",
      "2024-06-11T16:09:56,349 [WARN ] W-model-3-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "2024-06-11T16:09:56,350 [WARN ] W-model-3-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\n",
      "2024-06-11T16:09:56,430 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 82\n",
      "2024-06-11T16:09:56,429 [INFO ] W-model-3-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04029273986816406 ms\n",
      "2024-06-11T16:09:56,433 [INFO ] W-model-3-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 79.54978942871094 ms\n",
      "2024-06-11T16:09:56,433 [INFO ] W-model-3-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.1819133758544922 ms\n",
      "2024-06-11T16:09:56,433 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:47714 \"POST /invocations HTTP/1.1\" 200 90\n",
      "2024-06-11T16:09:56,497 [WARN ] W-model-2-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "2024-06-11T16:09:56,498 [WARN ] W-model-2-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\n",
      "2024-06-11T16:09:56,575 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.03528594970703125 ms\n",
      "2024-06-11T16:09:56,576 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 80\n",
      "2024-06-11T16:09:56,576 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 78.60398292541504 ms\n",
      "2024-06-11T16:09:56,576 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:47714 \"POST /invocations HTTP/1.1\" 200 81\n",
      "2024-06-11T16:09:56,577 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.13589859008789062 ms\n",
      "2024-06-11T16:09:56,586 [WARN ] W-model-4-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "2024-06-11T16:09:56,586 [WARN ] W-model-4-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\n",
      "2024-06-11T16:09:56,661 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 76\n",
      "2024-06-11T16:09:56,661 [INFO ] W-model-4-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04315376281738281 ms\n",
      "2024-06-11T16:09:56,662 [INFO ] W-model-4-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 75.00076293945312 ms\n",
      "2024-06-11T16:09:56,662 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:47714 \"POST /invocations HTTP/1.1\" 200 78\n",
      "2024-06-11T16:09:56,662 [INFO ] W-model-4-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.1323223114013672 ms\n",
      "2024-06-11T16:09:56,669 [WARN ] W-model-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "2024-06-11T16:09:56,670 [WARN ] W-model-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\n",
      "2024-06-11T16:09:56,745 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.0324249267578125 ms\n",
      "2024-06-11T16:09:56,745 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 77\n",
      "2024-06-11T16:09:56,746 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 75.6845474243164 ms\n",
      "2024-06-11T16:09:56,746 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.15997886657714844 ms\n",
      "2024-06-11T16:09:56,746 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:47714 \"POST /invocations HTTP/1.1\" 200 79\n",
      "2024-06-11T16:09:56,940 [INFO ] W-model-3-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04792213439941406 ms\n",
      "2024-06-11T16:09:56,940 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 187\n",
      "2024-06-11T16:09:56,941 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:47714 \"POST /invocations HTTP/1.1\" 200 189\n",
      "2024-06-11T16:09:56,941 [INFO ] W-model-3-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 186.47074699401855 ms\n",
      "2024-06-11T16:09:56,942 [INFO ] W-model-3-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.12230873107910156 ms\n",
      "2024-06-11T16:09:56.244:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=SINGLE_RECORD\n"
     ]
    }
   ],
   "source": [
    "# source: https://github.com/huggingface/notebooks/blob/main/sagemaker/12_batch_transform_inference/sagemaker-notebook.ipynb\n",
    "# parameters source: https://discuss.huggingface.co/t/errors-while-running-a-sagemaker-batch-transform-inference-job/38598\n",
    "output_s3_path = \"s3://sector-classification-aiml/batch_transform/output\"\n",
    "s3_file_uri = \"s3://sector-classification-aiml/batch_transform/input/data.jsonl\"\n",
    "\n",
    "# create Transformer to run our batch job\n",
    "batch_job = huggingface_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    output_path=output_s3_path, # we are using the same s3 path to save the output with the input\n",
    "    strategy='SingleRecord')\n",
    "\n",
    "# starts batch transform job and uses s3 data as input\n",
    "batch_job.transform(\n",
    "    data=s3_file_uri,\n",
    "    content_type='application/json',    \n",
    "    split_type='Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_sector_class_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
