{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import awswrangler as wr\n",
    "import numpy as np\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from botocore.exceptions import ClientError\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_txt_file(txt_obj, path):\n",
    "    \"\"\"Saves text file locally\"\"\"\n",
    "    with open(path, \"w\") as text_file:\n",
    "        text_file.write(txt_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_metadata_file(data, path):\n",
    "    \"\"\"Saves json obj locally\"\"\"        \n",
    "    with open(path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upload_file_to_s3(file_path, bucket_name, s3_path, max_retries=3):\n",
    "    \"\"\"\n",
    "    Uploads a single file to an S3 bucket with retries on failure.\n",
    "\n",
    "    :param file_path: Path to the file to upload\n",
    "    :param bucket_name: S3 bucket name\n",
    "    :param s3_path: Full S3 path for the file to be uploaded to\n",
    "    :param max_retries: Maximum number of retries on failure\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    attempts = 0\n",
    "    while attempts < max_retries:\n",
    "        try:\n",
    "            s3_client.upload_file(file_path, bucket_name, s3_path)\n",
    "            # print(f\"Uploaded {file_path} to {s3_path}\")\n",
    "            break  # Break out of the loop on success\n",
    "        except Exception as e:\n",
    "            attempts += 1\n",
    "            # print(f\"Error uploading {file_path}: {e}. Attempt {attempts} of {max_retries}\")\n",
    "            if attempts < max_retries:\n",
    "                sleep_time = 2 ** attempts  # Exponential back-off\n",
    "                # print(f\"Retrying in {sleep_time} seconds...\")\n",
    "                sleep(sleep_time)\n",
    "            else:\n",
    "                print(f\"Failed to upload {file_path} after {max_retries} attempts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_folder_to_s3_concurrently(folder_path, bucket_name, s3_folder, max_workers=100):\n",
    "    \"\"\"\n",
    "    Uploads a folder to an S3 bucket using multiple threads for efficiency.\n",
    "\n",
    "    :param folder_path: Local path to the folder\n",
    "    :param bucket_name: S3 bucket name\n",
    "    :param s3_folder: Folder path in S3 bucket\n",
    "    :param max_workers: Maximum number of threads\n",
    "    \"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        for subdir, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                full_path = os.path.join(subdir, file)\n",
    "                s3_path = os.path.join(s3_folder, os.path.relpath(full_path, folder_path))\n",
    "                futures.append(executor.submit(upload_file_to_s3, full_path, bucket_name, s3_path))\n",
    "\n",
    "        for idx, future in enumerate(as_completed(futures)):\n",
    "            if idx % max_workers==0:\n",
    "                print(idx)\n",
    "            # Wait for all futures to complete, can handle results or exceptions here\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICMS Issuer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Load icms_issuer data\n",
    "SCHEMA_NAME = \"datalake-curated-production\"\n",
    "\n",
    "query_stry = \"\"\"\n",
    "SELECT * \n",
    "FROM icms_issuer \n",
    "where 1=1\n",
    "and exclude_from_data_products = False\n",
    "\"\"\"\n",
    "\n",
    "df = wr.athena.read_sql_query(sql=query_stry, database=SCHEMA_NAME)\n",
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_issuer_paragraph(row)->str:\n",
    "    \"\"\"Generate issuer string paragraph containing details from structured data\"\"\"\n",
    "    description = row['description']\n",
    "    year_founded = row['year_founded']\n",
    "    lifecycle_status = row['lifecycle_status']\n",
    "    domicile_country_code = row['domicile_country_code']\n",
    "    domicile_state_code = row['domicile_state_code']\n",
    "    legal_entity_name = row['legal_entity_name']\n",
    "    slug = row['slug']\n",
    "    is_banner_visible = row['is_banner_visible']\n",
    "    banner_message = row['banner_message']\n",
    "    search_aliases = row['search_aliases']\n",
    "    stock_split_message = row['stock_split_message']\n",
    "    exited_date = row['exited_date']\n",
    "    name = row['name']\n",
    "    sector = row['sector']\n",
    "    sub_sector = row['sub_sector']\n",
    "    \n",
    "    \n",
    "    name_str = f\"{name} ({slug}): \"\n",
    "    description_str=f\"\"\"{f\"{description}\" if not pd.isna(description) else \"\"}\"\"\"\n",
    "    sector_str=f\"\"\"{f\"It operates in the {sector} sector and more specifically the {sub_sector} sectors.\" if not pd.isna(sub_sector) else \"\"}\"\"\"\n",
    "    country_str=f\"\"\"{f\"It is domiciled in the {domicile_country_code}.\" if not pd.isna(domicile_country_code) else \"\"}\"\"\"\n",
    "    state_str=f\"\"\"{f\"The state it is domiciled in is {domicile_state_code}.\" if not pd.isna(domicile_state_code) else \"\"}\"\"\"\n",
    "    banner_message_str=f\"\"\"{f\"{banner_message}\" if not pd.isna(banner_message) else \"\"}\"\"\"\n",
    "    stock_split_message_str=f\"\"\"{f\"{stock_split_message}\" if not pd.isna(stock_split_message) else \"\"}\"\"\"\n",
    "    \n",
    "    aliases_str = \"\"\n",
    "    try:  \n",
    "        aliases_str += \"The company can also be known as: \"\n",
    "        aliases_list = []\n",
    "        for alias in eval(search_aliases):\n",
    "            aliases_list.append(alias.strip())\n",
    "\n",
    "        aliases_str += f\"{str(aliases_list)}.\"\n",
    "    except:\n",
    "        aliases_str = \"\"\n",
    "        \n",
    "    \n",
    "    str_list = [name_str, description_str, sector_str, country_str, state_str, aliases_str, banner_message_str, stock_split_message_str]\n",
    "    issuer_str = \"\"\n",
    "    for s in str_list:\n",
    "        if s != \"\":\n",
    "            issuer_str += f\" {s}\"\n",
    "        \n",
    "    return issuer_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating paragraph for each issuer and saving file and metadata locally\n",
    "for idx, row in df.iterrows():\n",
    "    \n",
    "    issuer_para = get_issuer_paragraph(row)\n",
    "    name = row['name']\n",
    "    slug = row[\"slug\"]\n",
    "    country = row[\"domicile_country_code\"]\n",
    "    year_founded = row[\"year_founded\"]\n",
    "    sector = row[\"sector\"]\n",
    "    sub_sector = row[\"sub_sector\"]\n",
    "\n",
    "    metadata = {\n",
    "        \"metadataAttributes\": {\n",
    "            \"company_name\": f\"\"\"{f\"{name}\" if not pd.isna(name) else \"N/A\"}\"\"\",\n",
    "            \"company_id\": f\"\"\"{f\"{slug}\" if not pd.isna(slug) else \"N/A\"}\"\"\",\n",
    "            \"country\": f\"\"\"{f\"{country}\" if not pd.isna(country) else \"N/A\"}\"\"\",\n",
    "            \"year_founded\": f\"\"\"{f\"{year_founded}\" if not pd.isna(year_founded) else \"N/A\"}\"\"\",\n",
    "            \"sector\": f\"\"\"{f\"{sector}\" if not pd.isna(sector) else \"N/A\"}\"\"\",\n",
    "            \"sub_sector\": f\"\"\"{f\"{sub_sector}\" if not pd.isna(sub_sector) else \"N/A\"}\"\"\",\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # if idx % 1000==0:\n",
    "    #     print(idx)\n",
    "\n",
    "    filepath = f\"outputs/issuer_info/{slug}.txt\"\n",
    "    metadata_ext = \".metadata.json\"\n",
    "    save_txt_file_s3(issuer_para, filepath)\n",
    "    save_json_metadata_file(metadata, f\"{filepath}{metadata_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading files to S3 concurrently to increase latency\n",
    "folder_path = 'outputs/issuer_info'\n",
    "bucket_name = 'knowledge-base-aiml-test'\n",
    "s3_folder = 'issuer_info'\n",
    "\n",
    "upload_folder_to_s3_concurrently(folder_path, bucket_name, s3_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load key people data\n",
    "SCHEMA_NAME = \"datalake-curated-production\"\n",
    "\n",
    "query_stry = \"\"\"\n",
    "SELECT \n",
    "    ii.name as issuer_name\n",
    "    , kp.* \n",
    "FROM icms_issuer_key_person kp\n",
    "join icms_issuer ii\n",
    "on ii.slug=kp.issuer_slug\n",
    "where 1=1\n",
    "and ii.exclude_from_data_products = false\n",
    "\"\"\"\n",
    "\n",
    "df_kp = wr.athena.read_sql_query(sql=query_stry, database=SCHEMA_NAME)\n",
    "# df_kp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_employees(df, issuer_slug, issuer_name)->str:\n",
    "    \"\"\"Generate key employee string\"\"\"\n",
    "    ke = df[(df['issuer_slug']==issuer_slug) & (df['role_type']=='KeyEmployee')].reset_index()\n",
    "    if len(ke)>0:\n",
    "        key_employee_str=f\"The key employees at {issuer_name} ({issuer_slug}) include: \"\n",
    "        for idx, row in ke.iterrows():\n",
    "            name = row['name']\n",
    "            context = row['context']\n",
    "            \n",
    "            if idx == 0:\n",
    "                name_context_str = f\"{name} ({context})\"\n",
    "            else:\n",
    "                name_context_str = f\", {name} ({context})\"\n",
    "                \n",
    "            key_employee_str += name_context_str\n",
    "        # print(key_employee_str)\n",
    "        return key_employee_str\n",
    "    else:\n",
    "        # print(key_employee_str)\n",
    "        return \"\"\n",
    "    \n",
    "       \n",
    "def get_board_members(df, issuer_slug, issuer_name)->str:\n",
    "    \"\"\"Generate board members string\"\"\"\n",
    "    bm = df[(df['issuer_slug']==issuer_slug) & (df['role_type']=='BoardMember')].reset_index()      \n",
    "    if len(bm)>0:\n",
    "        board_members_str=f\"The Board Members at {issuer_name} ({issuer_slug}) include: \"\n",
    "        for idx, row in bm.iterrows():\n",
    "            name = row['name']\n",
    "            context = row['context']\n",
    "            \n",
    "            if idx == 0:\n",
    "                name_context_str = f\"{name} ({context})\"\n",
    "            else:\n",
    "                name_context_str = f\", {name} ({context})\"\n",
    "                \n",
    "            board_members_str += name_context_str\n",
    "        # print(board_members_str)    \n",
    "        return board_members_str\n",
    "    else:\n",
    "        # print(board_members_str)\n",
    "        return \"\"\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating paragraph for key people and board members for each issuer and saving file and metadata locally\n",
    "\n",
    "issuer_list = list(set(zip(df_kp['issuer_name'], df_kp['issuer_slug'])))\n",
    "for issuer in issuer_list:\n",
    "    issuer_name = issuer[0]\n",
    "    issuer_slug = issuer[1]\n",
    "    \n",
    "    kp_str = get_key_employees(df_kp, issuer_slug, issuer_name)\n",
    "    \n",
    "    metadata = {\n",
    "        \"metadataAttributes\": {\n",
    "            \"company_name\": f\"\"\"{f\"{issuer_name}\" if not pd.isna(issuer_name) else \"N/A\"}\"\"\",\n",
    "            \"company_id\": f\"\"\"{f\"{issuer_slug}\" if not pd.isna(issuer_slug) else \"N/A\"}\"\"\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if kp_str != \"\":\n",
    "        filepath = f\"outputs/key_employees/{issuer_slug}.txt\"\n",
    "        save_txt_file_s3(kp_str, path=filepath)\n",
    "        save_json_metadata_file(metadata, f\"{filepath}{metadata_ext}\")\n",
    "    \n",
    "    bm_str = get_board_members(df_kp, issuer_slug, issuer_name)\n",
    "    if bm_str != \"\":\n",
    "        filepath = f\"outputs/board_members/{issuer_slug}.txt\"\n",
    "        save_txt_file_s3(bm_str, path=filepath)\n",
    "        save_json_metadata_file(metadata, f\"{filepath}{metadata_ext}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading files to S3 concurrently to increase latency\n",
    "\n",
    "folder_path = 'outputs/key_employees'\n",
    "bucket_name = 'knowledge-base-aiml-test'\n",
    "s3_folder = 'key_employees'\n",
    "\n",
    "upload_folder_to_s3_concurrently(folder_path, bucket_name, s3_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading files to S3 concurrently to increase latency\n",
    "\n",
    "folder_path = 'outputs/board_members'\n",
    "bucket_name = 'knowledge-base-aiml-test'\n",
    "s3_folder = 'board_members'\n",
    "\n",
    "upload_folder_to_s3_concurrently(folder_path, bucket_name, s3_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funding Rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from funding_rounds\n",
    "SCHEMA_NAME = \"datalake-curated-production\"\n",
    "\n",
    "query_stry = \"\"\"\n",
    "SELECT \n",
    "    ii.name as issuer_name\n",
    "    , fr.* \n",
    "FROM funding_rounds fr\n",
    "join icms_issuer ii\n",
    "on fr.issuer_id_name=ii.slug\n",
    "where 1=1\n",
    "    and ii.exclude_from_data_products = false\n",
    "\"\"\"\n",
    "\n",
    "df_fr = wr.athena.read_sql_query(sql=query_stry, database=SCHEMA_NAME)\n",
    "# df_fr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_funding_round_paragraph(row) -> str:\n",
    "    \"\"\"Generate frunding rounds string paragraph containing details from structured data\"\"\"\n",
    "    \n",
    "    name = row[\"issuer_name\"]\n",
    "    slug = row[\"issuer_id_name\"]\n",
    "    funding_date = row[\"funding_date\"]\n",
    "    implied_valuation_dollars = row[\"implied_valuation_dollars\"]\n",
    "    money_raised_dollars = row[\"money_raised_dollars\"]\n",
    "    number_of_shares = row[\"number_of_shares\"]\n",
    "    share_price_cents = row[\"share_price_cents\"]\n",
    "    share_type = row[\"share_type\"]\n",
    "\n",
    "    conversion_ratio = row[\"conversion_ratio\"]\n",
    "    liquidation_preference = row[\"liquidation_preference\"]\n",
    "    liquidation_preference_order = row[\"liquidation_preference_order\"]\n",
    "    participation = row[\"participation\"]\n",
    "    participation_cap = row[\"participation_cap\"]\n",
    "\n",
    "    dividend_percent = row[\"dividend_percent\"]\n",
    "    dividend_rate = row[\"dividend_rate\"]\n",
    "    dividend_authorized = row[\"dividend_authorized\"]\n",
    "    dividend_cumulative = row[\"dividend_cumulative\"]\n",
    "    dividend_type = row[\"dividend_type\"]\n",
    "    blocking_right = row[\"blocking_right\"]\n",
    "\n",
    "    investors = row[\"investors\"]\n",
    "\n",
    "    name_str = f\"{name} ({slug})\"\n",
    "    funding_date_str = (\n",
    "        f\"\"\"{f\"{funding_date.date()}\" if not pd.isna(funding_date) else \"\"}\"\"\"\n",
    "    )\n",
    "    implied_valuation_dollars_str = f\"\"\"{\"\" if (pd.isna(implied_valuation_dollars) or implied_valuation_dollars==0) else f\"${implied_valuation_dollars:,.0f}\"}\"\"\"\n",
    "    money_raised_dollars_str = f\"\"\"{\"\" if (pd.isna(money_raised_dollars) or money_raised_dollars==0) else f\"${money_raised_dollars:,.0f}\"}\"\"\"\n",
    "    number_of_shares_common_str = f\"\"\"{\"\" if (pd.isna(number_of_shares) or number_of_shares==0) else f\"{number_of_shares:,.0f}\"}\"\"\"\n",
    "    number_of_shares_str = f\"\"\"{\"\" if (pd.isna(number_of_shares) or number_of_shares==0) else f\"Number of shares: {number_of_shares:,.0f}\"}\"\"\"\n",
    "    share_price_str = f\"\"\"{\"\" if (pd.isna(share_price_cents) or share_price_cents==0) else f\"Share price: ${share_price_cents/100:,.2f}\"}\"\"\"\n",
    "    share_type_str = f\"\"\"{f\"{share_type}\" if not pd.isna(share_type) else \"\"}\"\"\"\n",
    "    conversion_ratio_str = f\"\"\"{\"\" if (pd.isna(conversion_ratio) or conversion_ratio==0) else f\"Conversion Ratio: {conversion_ratio:.2f}\"}\"\"\"\n",
    "    liquidation_preference_str = f\"\"\"{\"\" if (pd.isna(liquidation_preference) or liquidation_preference==0) else f\"Liquidation Preference: {liquidation_preference:.2f}\"}\"\"\"\n",
    "    liquidation_preference_order_str = f\"\"\"{\"\" if (pd.isna(liquidation_preference_order) or liquidation_preference_order==0) else f\"Liquidation Preference Order: {liquidation_preference_order:.0f}\"}\"\"\"\n",
    "    participation_str = f\"\"\"{f\"Participation Rights: {participation}\" if not pd.isna(participation) else \"\"}\"\"\"\n",
    "    participation_cap = f\"\"\"{\"\" if (pd.isna(participation_cap) or participation_cap==0) else f\"Participation Cap: {participation_cap:.2f}\"}\"\"\"\n",
    "    dividend_percent_str = f\"\"\"{\"\" if (pd.isna(dividend_percent) or dividend_percent==0) else f\"Dividend percentage: {dividend_percent:.2f}\"}\"\"\"\n",
    "    dividend_rate_str = f\"\"\"{\"\" if (pd.isna(dividend_rate) or dividend_rate==0) else f\"Dividend Rate; {dividend_rate:.2f}\"}\"\"\"\n",
    "    dividend_authorized_str = f\"\"\"{f\"Dividend Authorized: {dividend_authorized}\" if not pd.isna(dividend_authorized) else \"\"}\"\"\"\n",
    "    dividend_cumulative_str = f\"\"\"{f\"Dividend cumulative: {dividend_cumulative}\" if not pd.isna(dividend_cumulative) else \"\"}\"\"\"\n",
    "    dividend_type_str = (\n",
    "        f\"\"\"{f\"Dividend type: {dividend_type}\" if not pd.isna(dividend_type) else \"\"}\"\"\"\n",
    "    )\n",
    "    blocking_right_str = f\"\"\"{f\"Blocking rights: {blocking_right}\" if not pd.isna(blocking_right) else \"\"}\"\"\"\n",
    "\n",
    "    try:\n",
    "        investors_list = eval(investors)\n",
    "        if len(investors_list) > 0:\n",
    "            investors_str = \"Investors: \"\n",
    "            for i, investor in enumerate(investors_list):\n",
    "                if i == 0:\n",
    "                    investors_str += investor\n",
    "                else:\n",
    "                    investors_str += f\", {investor}\"\n",
    "\n",
    "        else:\n",
    "            investors_str = \"\"\n",
    "    except:\n",
    "        investors_str = \"\"\n",
    "\n",
    "    if re.search(r\"Common\", share_type):\n",
    "        output_str = f\"As of {funding_date_str}, {name_str} has {number_of_shares_common_str} {share_type_str} shares. Below are the other details related to the shares:\"\n",
    "    else:\n",
    "        output_str = f\"{name_str} raised {money_raised_dollars_str} in a {share_type_str} funding round on {funding_date_str}, implying a valuation of {implied_valuation_dollars_str}. Below are the details from the funding round:\"\n",
    "\n",
    "    obj_list = [\n",
    "        investors_str,\n",
    "        number_of_shares_str,\n",
    "        share_price_str,\n",
    "        conversion_ratio_str,\n",
    "        liquidation_preference_str,\n",
    "        liquidation_preference_order_str,\n",
    "        participation_str,\n",
    "        participation_cap,\n",
    "        dividend_percent_str,\n",
    "        dividend_rate_str,\n",
    "        dividend_authorized_str,\n",
    "        dividend_cumulative_str,\n",
    "        dividend_type_str,\n",
    "        blocking_right_str,\n",
    "    ]\n",
    "\n",
    "    for obj in obj_list:\n",
    "        if obj != \"\":\n",
    "            output_str += f\"\\n {obj}\"\n",
    "\n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating paragraph for each issuer and saving file and metadata locally\n",
    "\n",
    "for idx, row in df_fr.iterrows():\n",
    "    \n",
    "    funding_round_para = get_funding_round_paragraph(row)\n",
    "    \n",
    "    name = row['issuer_name']\n",
    "    slug = row['issuer_id_name']\n",
    "    share_type = row['share_type']\n",
    "    funding_date = row['funding_date']\n",
    "\n",
    "    metadata = {\n",
    "        \"metadataAttributes\": {\n",
    "            \"company_name\": f\"\"\"{f\"{name}\" if not pd.isna(name) else \"N/A\"}\"\"\",\n",
    "            \"company_id\": f\"\"\"{f\"{slug}\" if not pd.isna(slug) else \"N/A\"}\"\"\",\n",
    "            \"share_type\": f\"\"\"{f\"{share_type}\" if not pd.isna(share_type) else \"N/A\"}\"\"\",\n",
    "            \"funding_date\": f\"\"\"{f\"{funding_date.date()}\" if not pd.isna(funding_date) else \"N/A\"}\"\"\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # if idx % 1000 == 0:\n",
    "    #     print(idx)\n",
    "    \n",
    "    filepath = f'outputs/funding_rounds/\"{slug}_{share_type}_{funding_date.date()}.txt'\n",
    "    save_txt_file_s3(funding_round_para, path=filepath)\n",
    "    save_json_metadata_file(metadata, f\"{filepath}{metadata_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading files to S3 concurrently to increase latency\n",
    "folder_path = 'outputs/funding_rounds'\n",
    "bucket_name = 'knowledge-base-aiml-test'\n",
    "s3_folder = 'funding_rounds'\n",
    "\n",
    "upload_folder_to_s3_concurrently(folder_path, bucket_name, s3_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Public Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load public marks data\n",
    "SCHEMA_NAME = \"datalake-curated-production\"\n",
    "\n",
    "query_stry = \"\"\"\n",
    "SELECT \n",
    "    ii.name,\n",
    "    pm.*\n",
    "FROM public_marks pm\n",
    "join icms_issuer ii\n",
    "on pm.issuer_id_name=ii.slug\n",
    "where 1=1\n",
    " and ii.exclude_from_data_products = false\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "df_pm = wr.athena.read_sql_query(sql=query_stry, database=SCHEMA_NAME)\n",
    "# df_pm.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_public_marks_paragraph(row)->str:\n",
    "    \"\"\"Generate public marks string paragraph containing details from structured data\"\"\"\n",
    "    name = row[\"name\"]\n",
    "    slug = row[\"issuer_id_name\"]\n",
    "    reported_on = row['reported_on']\n",
    "    share_price_cents = row['price_cents']\n",
    "    implied_enterprise_value_cents = row['implied_enterprise_value_cents']\n",
    "    share_type = row['share_type']\n",
    "    fund_name = row['fund_name']\n",
    "    fund_ticker = row['fund_ticker']\n",
    "    fund_family = row['fund_family']\n",
    "    source_url = row['url']\n",
    "    is_after_exit = row['is_after_exit']\n",
    "    \n",
    "    name_str = f\"{name} ({slug})\"\n",
    "    reported_on_str = (\n",
    "        f\"\"\"{f\"{reported_on.date()}\" if not pd.isna(reported_on) else \"\"}\"\"\"\n",
    "    )\n",
    "    share_price_str = f\"\"\"{\"\" if (pd.isna(share_price_cents) or share_price_cents==0) else f\"${share_price_cents/100:,.2f}\"}\"\"\"\n",
    "    implied_enterprise_value_str = f\"\"\"{\"\" if (pd.isna(implied_enterprise_value_cents) or implied_enterprise_value_cents==0) else f\"The implied enterprise value based on the share price is ${implied_enterprise_value_cents/100:,.0f}.\"}\"\"\"\n",
    "    share_type_str = f\"\"\"{f\"{share_type}\" if not pd.isna(share_type) else \"\"}\"\"\"\n",
    "    fund_name_str = f\"\"\"{f\"{fund_name}\" if not pd.isna(fund_name) else \"\"}\"\"\"\n",
    "    fund_ticker_str = f\"\"\"{f\"{fund_ticker}\" if not pd.isna(fund_ticker) else \"\"}\"\"\"\n",
    "    fund_family_str = f\"\"\"{f\"{fund_family}\" if not pd.isna(fund_family) else \"\"}\"\"\"\n",
    "    source_url_str = f\"\"\"{f\"Source: {source_url}\" if not pd.isna(source_url) else \"\"}\"\"\"\n",
    "    is_after_exit_str = f\"\"\"{\"Note this was after the company exited.\" if is_after_exit else \"\"}\"\"\"\n",
    "    \n",
    "    \n",
    "    public_marks_str = f\"On {reported_on_str}, {fund_family_str} ({fund_name_str}) reported a mututal fund public mark on it's investment in {name_str}, pricing its {share_type_str} shares at {share_price_str} per share.\"\n",
    "    \n",
    "    str_list = [implied_enterprise_value_str, is_after_exit_str, source_url_str]\n",
    "    for s in str_list:\n",
    "        if s != \"\":\n",
    "            public_marks_str += f\" {s}\"\n",
    "            \n",
    "    # print(public_marks_str)\n",
    "    return public_marks_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating paragraph for each issuer and saving file and metadata locally\n",
    "\n",
    "for idx, row in df_pm.iterrows():\n",
    "    \n",
    "    public_marks_para = get_public_marks_paragraph(row)\n",
    "    name = row[\"name\"]\n",
    "    slug = row[\"issuer_id_name\"]\n",
    "    id = row[\"id\"]\n",
    "    reported_on = row['reported_on']\n",
    "    fund_ticker = row['fund_ticker']\n",
    "    fund_family = row['fund_family']\n",
    "    is_after_exit = row['is_after_exit']    \n",
    "    \n",
    "    metadata = {\n",
    "        \"metadataAttributes\": {\n",
    "            \"company_name\": f\"\"\"{f\"{name}\" if not pd.isna(name) else \"N/A\"}\"\"\",\n",
    "            \"company_id\": f\"\"\"{f\"{slug}\" if not pd.isna(slug) else \"N/A\"}\"\"\",\n",
    "            \"reported_on\": f\"\"\"{f\"{reported_on.date()}\" if not pd.isna(reported_on) else \"N/A\"}\"\"\",\n",
    "            \"fund_family\": f\"\"\"{f\"{fund_family}\" if not pd.isna(fund_family) else \"N/A\"}\"\"\",\n",
    "            \"is_after_exit\": f\"\"\"{f\"{is_after_exit}\" if not pd.isna(is_after_exit) else \"N/A\"}\"\"\",\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # if idx % 5000 == 0:\n",
    "    #     print(idx)\n",
    "    \n",
    "    filepath = f\"outputs/public_marks/{slug}_{id}_{reported_on.date()}.txt\"\n",
    "    metadata_ext = \".metadata.json\"    \n",
    "    save_txt_file_s3(public_marks_para, path=filepath)\n",
    "    save_json_metadata_file(metadata, f\"{filepath}{metadata_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading files to S3 concurrently to increase latency\n",
    "folder_path = 'outputs/public_marks'\n",
    "bucket_name = 'knowledge-base-aiml-test'\n",
    "s3_folder = 'public_marks'\n",
    "\n",
    "upload_folder_to_s3_concurrently(folder_path, bucket_name, s3_folder, max_workers=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secondary transactions data\n",
    "SCHEMA_NAME = \"datalake-curated-production\"\n",
    "\n",
    "query_stry = \"\"\"\n",
    "SELECT \n",
    "    ii.name\n",
    "    , st.*\n",
    "FROM secondary_transactions st\n",
    "join icms_issuer ii\n",
    "on st.issuer_id_name=ii.slug\n",
    "where 1=1\n",
    " and ii.exclude_from_data_products = false\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "df_st = wr.athena.read_sql_query(sql=query_stry, database=SCHEMA_NAME)\n",
    "# df_st.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_secondary_transactions_paragraph(row)->str:\n",
    "    \"\"\"Generate secondary transactions string paragraph containing details from structured data\"\"\"\n",
    "    name = row[\"name\"]\n",
    "    slug = row[\"issuer_id_name\"]\n",
    "    closed_date = row['closed_date']\n",
    "    share_class = row['class']\n",
    "    number_of_shares = row['number_of_shares']\n",
    "    price_per_share_cents = row['price_per_share_cents']\n",
    "    transaction_size_cents = row['transaction_size_cents']\n",
    "    structure = row['structure']\n",
    "    rofr = row['rofr']\n",
    "    source_created_at = row['source_created_at']\n",
    "    source_updated_at = row['source_updated_at']\n",
    "    execution_venue = row['execution_venue']\n",
    "    buyer_type = row['buyer_type']\n",
    "    seller_type = row['seller_type']\n",
    "    \n",
    "    name_str = f\"{name} ({slug})\"\n",
    "    share_class_str = f\"\"\"{f\"{share_class}\" if not pd.isna(share_class) else \"\"}\"\"\"\n",
    "    structure_str = f\"\"\"{f\"Structure: {structure}\" if not pd.isna(structure) else \"\"}\"\"\"\n",
    "    rofr_str = f\"\"\"{f\"Right of first refusal: {rofr}\" if not pd.isna(rofr) else \"\"}\"\"\"\n",
    "    closed_date_str = (\n",
    "        f\"\"\"{f\"{closed_date.date()}\" if not pd.isna(closed_date) else \"\"}\"\"\"\n",
    "    )\n",
    "    source_created_at_str = (\n",
    "        f\"\"\"{f\"{source_created_at.date()}\" if not pd.isna(source_created_at) else \"\"}\"\"\"\n",
    "    )\n",
    "    source_updated_at_str = (\n",
    "        f\"\"\"{f\"{source_updated_at.date()}\" if not pd.isna(source_updated_at) else \"\"}\"\"\"\n",
    "    )\n",
    "    price_per_share_str = f\"\"\"{\"\" if (pd.isna(price_per_share_cents) or price_per_share_cents==0) else f\"${price_per_share_cents/100:,.2f}\"}\"\"\"\n",
    "    transaction_size_str = f\"\"\"{\"\" if (pd.isna(transaction_size_cents) or transaction_size_cents==0) else f\"Total transaction size: ${transaction_size_cents/100:,.2f}\"}\"\"\"\n",
    "    number_of_shares_str = f\"\"\"{\"\" if (pd.isna(number_of_shares) or number_of_shares==0) else f\"{number_of_shares:,.0f}\"}\"\"\"\n",
    "    execution_venue_str = f\"\"\"{f\"Execution venue: {execution_venue}\" if not pd.isna(execution_venue) else \"\"}\"\"\"\n",
    "    buyer_type_str = f\"\"\"{f\"Buyer: {buyer_type}\" if not pd.isna(buyer_type) else \"\"}\"\"\"\n",
    "    seller_type_str = f\"\"\"{f\"Seller: {seller_type}\" if not pd.isna(seller_type) else \"\"}\"\"\"\n",
    "    \n",
    "    secondary_transactions_str = f\"On {closed_date_str}, {number_of_shares_str} {share_class_str} shares of {name_str} were purchased for {price_per_share_str} per share. Transaction details from the trade below:\"\n",
    "    \n",
    "    str_list = [transaction_size_str, rofr_str, buyer_type_str, seller_type_str, structure_str, execution_venue_str]\n",
    "    for s in str_list:\n",
    "        if s != \"\":\n",
    "            secondary_transactions_str += f\"\\n{s}\"\n",
    "            \n",
    "    # print(secondary_transactions_str)\n",
    "    return secondary_transactions_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating paragraph for each issuer and saving file and metadata locally\n",
    "\n",
    "for idx, row in df_st.iterrows():\n",
    "    \n",
    "    secondary_transactions_para = get_secondary_transactions_paragraph(row)\n",
    "    \n",
    "    transaction_id = row[\"id\"].split('/')[0]\n",
    "    name = row[\"name\"]\n",
    "    slug = row[\"issuer_id_name\"]\n",
    "    closed_date = row['closed_date']\n",
    "    transaction_size_cents = row['transaction_size_cents']\n",
    "    structure = row['structure']\n",
    "    rofr = row['rofr']\n",
    "    buyer_type = row['buyer_type']\n",
    "    seller_type = row['seller_type']  \n",
    "    \n",
    "    metadata = {\n",
    "        \"metadataAttributes\": {\n",
    "            \"company_name\": f\"\"\"{f\"{name}\" if not pd.isna(name) else \"N/A\"}\"\"\",\n",
    "            \"company_id\": f\"\"\"{f\"{slug}\" if not pd.isna(slug) else \"N/A\"}\"\"\",\n",
    "            \"transaction_size_str\": f\"\"\"{\"\" if (pd.isna(transaction_size_cents) or transaction_size_cents==0) else f\"{transaction_size_cents/100:,.2f}\"}\"\"\",\n",
    "            \"closed_date\": f\"\"\"{f\"{closed_date.date()}\" if not pd.isna(closed_date) else \"N/A\"}\"\"\",\n",
    "            \"rofr\": f\"\"\"{f\"{rofr}\" if not pd.isna(rofr) else \"N/A\"}\"\"\",\n",
    "            \"buyer_type\": f\"\"\"{f\"{buyer_type}\" if not pd.isna(buyer_type) else \"N/A\"}\"\"\",\n",
    "            \"seller_type\": f\"\"\"{f\"{seller_type}\" if not pd.isna(seller_type) else \"N/A\"}\"\"\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # if idx % 1000 == 0:\n",
    "    #     print(idx)\n",
    "    \n",
    "    filepath = f\"outputs/secondary_transactions/{slug}_{transaction_id}_{closed_date.date()}.txt\"\n",
    "    metadata_ext = \".metadata.json\"    \n",
    "    save_txt_file_s3(secondary_transactions_para, path=filepath)\n",
    "    save_json_metadata_file(metadata, f\"{filepath}{metadata_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading files to S3 concurrently to increase latency\n",
    "folder_path = 'outputs/secondary_transactions'\n",
    "bucket_name = 'knowledge-base-aiml-test'\n",
    "s3_folder = 'secondary_transactions'\n",
    "\n",
    "upload_folder_to_s3_concurrently(folder_path, bucket_name, s3_folder, max_workers=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VWAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VWAP data\n",
    "\n",
    "SCHEMA_NAME = \"datalake-curated-production\"\n",
    "\n",
    "query_stry = \"\"\"\n",
    "SELECT \n",
    "    ii.name\n",
    "    , vw.*\n",
    "FROM vwap vw\n",
    "join icms_issuer ii\n",
    "on vw.issuer_slug=ii.slug\n",
    "where 1=1\n",
    " and ii.exclude_from_data_products = false\n",
    " and vw.data_source = 'All'\n",
    " and vw.archived_at is null\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "df_vw = wr.athena.read_sql_query(sql=query_stry, database=SCHEMA_NAME)\n",
    "# df_vw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vwap_paragraph(row)->str:\n",
    "    \"\"\"Generate vwap string paragraph containing details from structured data\"\"\"\n",
    "    name = row[\"name\"]\n",
    "    slug = row[\"issuer_slug\"]\n",
    "    calc_date = row['calc_date']\n",
    "    share_type = row['share_type']\n",
    "    data_source = row['data_source']\n",
    "    vwap_7 = row['vwap_7']\n",
    "    vwap_30 = row['vwap_30']\n",
    "    vwap_45 = row['vwap_45']\n",
    "    vwap_60 = row['vwap_60']\n",
    "    vwap_90 = row['vwap_90']\n",
    "    \n",
    "    name_str = f\"{name} ({slug})\"\n",
    "    share_type_str = f\"\"\"{\"\" if pd.isna(share_type) or share_type == 'All' else f\"{share_type} \"}\"\"\"\n",
    "    data_source_str = f\"\"\"{f\"{data_source}\" if not pd.isna(data_source) else \"\"}\"\"\"\n",
    "    calc_date_str = (\n",
    "        f\"\"\"{f\"{calc_date.date()}\" if not pd.isna(calc_date) else \"\"}\"\"\"\n",
    "    )\n",
    "    vwap_7_str = f\"\"\"{\"\" if (pd.isna(vwap_7) or vwap_7==0) else f\"7-day vwap: {vwap_7:,.2f}\"}\"\"\"\n",
    "    vwap_30_str = f\"\"\"{\"\" if (pd.isna(vwap_30) or vwap_30==0) else f\"30-day vwap: {vwap_30:,.2f}\"}\"\"\"\n",
    "    vwap_45_str = f\"\"\"{\"\" if (pd.isna(vwap_45) or vwap_45==0) else f\"45-day vwap: {vwap_45:,.2f}\"}\"\"\"\n",
    "    vwap_60_str = f\"\"\"{\"\" if (pd.isna(vwap_60) or vwap_60==0) else f\"60-day vwap: {vwap_60:,.2f}\"}\"\"\"\n",
    "    vwap_90_str = f\"\"\"{\"\" if (pd.isna(vwap_90) or vwap_90==0) else f\"90-day vwap: {vwap_90:,.2f}\"}\"\"\"\n",
    "    \n",
    "    vwap_str = f\"On {calc_date_str}, the volume-weighted average price (VWAP) for {name_str} {share_type_str}shares was:\"\n",
    "    \n",
    "    str_list = [vwap_7_str, vwap_30_str, vwap_45_str, vwap_60_str, vwap_90_str]\n",
    "    for s in str_list:\n",
    "        if s != \"\":\n",
    "            vwap_str += f\"\\n{s}\"\n",
    "            \n",
    "    # print(vwap_str)\n",
    "    return vwap_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating paragraph for each issuer and saving file and metadata locally\n",
    "\n",
    "for idx, row in df_vw.iterrows():\n",
    "    \n",
    "    vwap_para = get_vwap_paragraph(row)\n",
    "    \n",
    "    slug = row[\"issuer_slug\"]\n",
    "    calc_date = row['calc_date']\n",
    "    share_type = row['share_type']\n",
    "    \n",
    "    metadata = {\n",
    "        \"metadataAttributes\": {\n",
    "            \"slug\": f\"\"\"{f\"{slug}\" if not pd.isna(slug) else \"N/A\"}\"\"\",\n",
    "            \"calc_date\": f\"\"\"{f\"{calc_date.date()}\" if not pd.isna(calc_date) else \"N/A\"}\"\"\",\n",
    "            \"share_type\": f\"\"\"{f\"{share_type}\" if not pd.isna(share_type) else \"N/A\"}\"\"\",\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # if idx == 1000:\n",
    "    #     print(idx)\n",
    "    \n",
    "    \n",
    "    filepath = f\"vwap/{slug}_{share_type}_{calc_date.date()}.txt\"\n",
    "    metadata_ext = \".metadata.json\"    \n",
    "    save_txt_file_s3(vwap_para, path=filepath)\n",
    "    save_json_metadata_file(metadata, f\"{filepath}{metadata_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading files to S3 concurrently to increase latency\n",
    "\n",
    "folder_path = 'outputs/vwap'\n",
    "bucket_name = 'knowledge-base-aiml-test'\n",
    "s3_folder = 'vwap'\n",
    "\n",
    "upload_folder_to_s3_concurrently(folder_path, bucket_name, s3_folder, max_workers=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forge Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Forge Prices data\n",
    "SCHEMA_NAME = \"datalake-curated-production\"\n",
    "\n",
    "query_stry = \"\"\"\n",
    "SELECT \n",
    "    ii.name\n",
    "    , fp.*\n",
    "FROM forge_prices fp\n",
    "join icms_issuer ii\n",
    "on fp.issuer_slug=ii.slug\n",
    "where 1=1\n",
    " and ii.exclude_from_data_products = false\n",
    "--and date >= date('2023-07-01')\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "df_fp = wr.athena.read_sql_query(sql=query_stry, database=SCHEMA_NAME)\n",
    "# df_fp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forge_prices_paragraph(row)->str:\n",
    "    \"\"\"Generate forge prices string paragraph containing details from structured data\"\"\"\n",
    "    name = row[\"name\"]\n",
    "    slug = row[\"issuer_slug\"]\n",
    "    date = row['date']\n",
    "    price = row['price']\n",
    "    \n",
    "    name_str = f\"{name} ({slug})\"\n",
    "    share_type_str = f\"\"\"{\"\" if pd.isna(share_type) or share_type == 'All' else f\"{share_type} \"}\"\"\"\n",
    "\n",
    "    date_str = (\n",
    "        f\"\"\"{f\"{date.date()}\" if not pd.isna(date) else \"\"}\"\"\"\n",
    "    )\n",
    "    price_str = f\"\"\"{\"\" if (pd.isna(price) or price==0) else f\"${price:,.2f}\"}\"\"\"\n",
    "    \n",
    "    forge_price_str = f\"The Forge Price for {name_str} on {date_str} was {price_str} per share.\"\n",
    "            \n",
    "    # print(forge_price_str)\n",
    "    return forge_price_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating paragraph for each issuer and saving file and metadata locally\n",
    "\n",
    "for idx, row in dff_fp.iterrows():\n",
    "    \n",
    "    forge_prices_para = get_forge_prices_paragraph(row)\n",
    "    \n",
    "    slug = row[\"issuer_slug\"]\n",
    "    date = row['date']\n",
    "    \n",
    "    metadata = {\n",
    "        \"metadataAttributes\": {\n",
    "            \"slug\": f\"\"\"{f\"{slug}\" if not pd.isna(slug) else \"N/A\"}\"\"\",\n",
    "            \"date\": f\"\"\"{f\"{date.date()}\" if not pd.isna(date) else \"N/A\"}\"\"\",\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    filepath = f\"forge_prices/{slug}_{date.date()}.txt\"\n",
    "    metadata_ext = \".metadata.json\"    \n",
    "    save_txt_file_s3(forge_prices_para, path=filepath)\n",
    "    save_json_metadata_file(metadata, f\"{filepath}{metadata_ext}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indications of Interest (IOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IOI data\n",
    "SCHEMA_NAME = \"datalake-curated-production\"\n",
    "\n",
    "query_stry = \"\"\"\n",
    "SELECT \n",
    "    ii.name\n",
    "    , ih.*\n",
    "FROM ioi_history ih\n",
    "join icms_issuer ii\n",
    "on ih.issuer_id_name=ii.slug\n",
    "where 1=1\n",
    " and ii.exclude_from_data_products = false\n",
    " and record_date >= date('2023-07-01')\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "df_ih = wr.athena.read_sql_query(sql=query_stry, database=SCHEMA_NAME)\n",
    "# df_ih.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ioi_paragraph(row)->str:\n",
    "    \"\"\"Generate IOI string paragraph containing details from structured data\"\"\"\n",
    "    name = row[\"name\"]\n",
    "    slug = row[\"issuer_id_name\"]\n",
    "    record_date = row['record_date']\n",
    "    created_at = row['created_at']\n",
    "    ioi_type = row['type']\n",
    "    share_class = row['class']\n",
    "    interests_share_type = row['interests_share_type']\n",
    "    min_volume = row['min_volume']\n",
    "    client_type = row['client_type']\n",
    "    max_volume = row['max_volume']\n",
    "    user_initiated = row['user_initiated']\n",
    "    min_price = row['min_price']\n",
    "    max_price = row['max_price']\n",
    "    deal_amount_min = row['deal_amount_min']\n",
    "    deal_amount_max = row['deal_amount_max']\n",
    "    implied_valuation_min = row['implied_valuation_min']\n",
    "    implied_valuation_max = row['implied_valuation_max']\n",
    "    \n",
    "    name_str = f\"{name} ({slug})\"\n",
    "    record_date_str = (\n",
    "        f\"\"\"{f\"{record_date.date()}\" if not pd.isna(record_date) else \"\"}\"\"\"\n",
    "    )\n",
    "    created_at_str = (\n",
    "        f\"\"\"{f\"{created_at.date()}\" if not pd.isna(created_at) else \"\"}\"\"\"\n",
    "    )\n",
    "    ioi_type_str = f\"\"\"{f\"{ioi_type}\" if not pd.isna(ioi_type) else \"\"}\"\"\"\n",
    "    share_class_str = f\"\"\"{f\"{share_class}\" if not pd.isna(share_class) else \"\"}\"\"\"\n",
    "    interests_share_type_str = f\"\"\"{f\"{interests_share_type}\" if not pd.isna(interests_share_type) else \"\"}\"\"\"\n",
    "    min_volume_str = f\"\"\"{\"\" if (pd.isna(min_volume) or min_volume==0) else f\"Min volume: {min_volume/100:,.2f}\"}\"\"\"\n",
    "    max_volume_str = f\"\"\"{\"\" if (pd.isna(max_volume) or max_volume==0) else f\"Max volume: {max_volume/100:,.2f}\"}\"\"\"\n",
    "    client_type_str = f\"\"\"{f\"{client_type}\" if not pd.isna(client_type) else \"\"}\"\"\"\n",
    "    user_initiated_str = f\"\"\"{f\"User initiated: {user_initiated}\" if not pd.isna(user_initiated) else \"\"}\"\"\"\n",
    "    min_price_str = f\"\"\"{\"\" if (pd.isna(min_price) or min_price==0) else f\"Min price: ${min_price/100:,.2f}\"}\"\"\"\n",
    "    max_price_str = f\"\"\"{\"\" if (pd.isna(max_price) or max_price==0) else f\"Max price: ${max_price/100:,.2f}\"}\"\"\"\n",
    "    deal_amount_min_str = f\"\"\"{\"\" if (pd.isna(deal_amount_min) or deal_amount_min==0) else f\"Min deal amount: ${deal_amount_min/100:,.2f}\"}\"\"\"\n",
    "    deal_amount_max_str = f\"\"\"{\"\" if (pd.isna(deal_amount_max) or deal_amount_max==0) else f\"Max deal amount: ${deal_amount_max/100:,.2f}\"}\"\"\"\n",
    "    implied_valuation_min_str = f\"\"\"{\"\" if (pd.isna(implied_valuation_min) or implied_valuation_min==0) else f\"Min implied valuation: ${implied_valuation_min/100:,.2f}\"}\"\"\"\n",
    "    implied_valuation_max_str = f\"\"\"{\"\" if (pd.isna(implied_valuation_max) or implied_valuation_max==0) else f\"Max implied valuation: ${implied_valuation_max/100:,.2f}\"}\"\"\"\n",
    "\n",
    "\n",
    "    ioi_str = f\"On {record_date_str}, an indication of interest (IOI) to {ioi_type.upper()} {name_str} {interests_share_type} shares was submitted by an {client_type} client. Below are further details on the IOI:\"\n",
    "    \n",
    "    str_list = [user_initiated_str, min_volume_str, max_volume_str,min_price_str, max_price_str, deal_amount_min_str, deal_amount_max_str,\n",
    "                implied_valuation_min_str, implied_valuation_max_str]\n",
    "    for s in str_list:\n",
    "        if s != \"\":\n",
    "            ioi_str += f\"\\n{s}\"\n",
    "            \n",
    "    # print(ioi_str)\n",
    "    return ioi_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating paragraph for each issuer and saving file and metadata locally\n",
    "\n",
    "for idx, row in df_ih.iterrows():\n",
    "    \n",
    "    ioi_para = get_ioi_paragraph(row)\n",
    "    \n",
    "    slug = row[\"issuer_id_name\"]\n",
    "    record_date = row['record_date']\n",
    "    ioi_id = row['ioi_id']\n",
    "    ioi_type = row['type']\n",
    "    \n",
    "    metadata = {\n",
    "        \"metadataAttributes\": {\n",
    "            \"slug\": f\"\"\"{f\"{slug}\" if not pd.isna(slug) else \"N/A\"}\"\"\",\n",
    "            \"ioi_type\": f\"\"\"{f\"{ioi_type}\" if not pd.isna(ioi_type) else \"N/A\"}\"\"\",\n",
    "            \"record_date\": f\"\"\"{f\"{record_date.date()}\" if not pd.isna(record_date) else \"N/A\"}\"\"\",\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # if idx % 1000==0:\n",
    "    #     print(idx)\n",
    "    \n",
    "    filepath = f\"outputs/ioi/{slug}_{ioi_id}_{record_date.date()}.txt\"\n",
    "    metadata_ext = \".metadata.json\"    \n",
    "    save_txt_file_s3(ioi_para, path=filepath)\n",
    "    save_json_metadata_file(metadata, f\"{filepath}{metadata_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading files to S3 concurrently to increase latency\n",
    "\n",
    "folder_path = 'outputs/ioi'\n",
    "bucket_name = 'knowledge-base-aiml-test'\n",
    "s3_folder = 'ioi'\n",
    "\n",
    "upload_folder_to_s3_concurrently(folder_path, bucket_name, s3_folder, max_workers=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
