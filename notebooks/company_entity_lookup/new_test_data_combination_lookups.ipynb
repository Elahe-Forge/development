{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_normalized = [\n",
    "        \"healthcare\",\n",
    "        \"technologies\",\n",
    "        \"therapeutics\",\n",
    "        \"financial\",\n",
    "        \"software\",\n",
    "        \"holdings\",\n",
    "        \"transportation\",\n",
    "        \"pharmaceuticals\",\n",
    "        \"capital\",\n",
    "        \"copper\",\n",
    "        \"communications\",\n",
    "        \"biotechnology\",\n",
    "        \"biopharmaceuticals\",\n",
    "        \"group\",\n",
    "        \"technology\",\n",
    "        \"media\",\n",
    "        \"energy\",\n",
    "        \"industries\",\n",
    "        \"biotherapeutics\",\n",
    "        \"solution\",\n",
    "        \"bioscience\",\n",
    "        \"industries\",\n",
    "        \"corporation\",\n",
    "        \"systems\",\n",
    "        \"enterprises\",\n",
    "        \"robotics\",\n",
    "        \"bank\",\n",
    "        \"inc\",\n",
    "        \"llc\",\n",
    "        \"pp\",\n",
    "        \"series a\",\n",
    "        \"series seed\",\n",
    "        \"series b\",\n",
    "        \"series c\",\n",
    "        \"series d\",\n",
    "        \"series e\",\n",
    "        \"series f\",\n",
    "        \"series g\",\n",
    "        \"series h\",\n",
    "        \"series i\"\n",
    "    ]\n",
    "\n",
    "def normalize_name(name):\n",
    "    name = name.lower() \n",
    "    for word in stop_words_normalized:\n",
    "        name = name.replace(word, '')\n",
    "    return name.strip()\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stop_words(sentence):\n",
    "    words = sentence.split()\n",
    "    filtered_sentence = [w for w in words if not w.lower() in stop_words]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "\n",
    "def remove_punctuation(sentence):\n",
    "    return sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "\n",
    "def clean_text(word_to_find):\n",
    "    word_to_find_normalzed = normalize_name(word_to_find)\n",
    "\n",
    "    word_to_find_no_stop_word = remove_stop_words(word_to_find_normalzed)\n",
    "\n",
    "    word_to_find_no_punc = remove_punctuation(word_to_find_no_stop_word)\n",
    "\n",
    "    return word_to_find_no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input string</th>\n",
       "      <th>slug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TAE Technologies</td>\n",
       "      <td>tae-technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tae</td>\n",
       "      <td>tae-technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tri Alpha Energy</td>\n",
       "      <td>tae-technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tri Alpha</td>\n",
       "      <td>tae-technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TAE Life Sciences</td>\n",
       "      <td>tae-technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TAE Power Management</td>\n",
       "      <td>tae-technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>X.AI Corp</td>\n",
       "      <td>xai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Grok</td>\n",
       "      <td>xai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Grok AI</td>\n",
       "      <td>xai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Xaira Therapeutics</td>\n",
       "      <td>xaira-therapeutics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Orion Medicines</td>\n",
       "      <td>xaira-therapeutics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Orionis</td>\n",
       "      <td>orionis-biosciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Orionis Biosciences</td>\n",
       "      <td>orionis-biosciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Orionis Bio</td>\n",
       "      <td>orionis-biosciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Yixia</td>\n",
       "      <td>yixia-com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Input string                 slug\n",
       "0       TAE Technologies     tae-technologies\n",
       "1                    tae     tae-technologies\n",
       "2       Tri Alpha Energy     tae-technologies\n",
       "3              Tri Alpha     tae-technologies\n",
       "4      TAE Life Sciences     tae-technologies\n",
       "5   TAE Power Management     tae-technologies\n",
       "6              X.AI Corp                  xai\n",
       "7                   Grok                  xai\n",
       "8                Grok AI                  xai\n",
       "9     Xaira Therapeutics   xaira-therapeutics\n",
       "10       Orion Medicines   xaira-therapeutics\n",
       "11               Orionis  orionis-biosciences\n",
       "12   Orionis Biosciences  orionis-biosciences\n",
       "13           Orionis Bio  orionis-biosciences\n",
       "14                 Yixia            yixia-com"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_test_data_df = pd.read_csv('new_test_data.csv')\n",
    "words_to_find = news_test_data_df['Input string']\n",
    "word_list = news_test_data_df['slug']\n",
    "news_test_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_company(name, master_list, top_n=5):\n",
    "    name = normalize_name(name)\n",
    "    \n",
    "    # Extract the top N matches along with their scores\n",
    "    ranked_matches = process.extract(name, master_list, scorer=fuzz.token_set_ratio, limit=top_n)\n",
    "    return ranked_matches if ranked_matches else []\n",
    "\n",
    "matched_companies = [match_company(c, word_list) for c in words_to_find]\n",
    "matched_companies_full_record = [(c, word_to_find) for c, word_to_find in zip(matched_companies, words_to_find)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items: 15\n",
      "Number of exact matches: 9\n",
      "Number of partial matches: 5\n",
      "Exact Match Accuracy: 60.00%\n",
      "Overall Accuracy (including partial matches): 93.33%\n"
     ]
    }
   ],
   "source": [
    "# Ranked matchings\n",
    "exact_matches = 0\n",
    "partial_matches = 0\n",
    "total_items = len(word_list)\n",
    "matched = []\n",
    "partial_matched = []\n",
    "not_matched = []\n",
    "\n",
    "for ranked_match_list, expected in zip(matched_companies_full_record, word_list):\n",
    "    if ranked_match_list:\n",
    "        first_match = ranked_match_list[0][0][0]  # Get the first-ranked match\n",
    "        nport_issuer_name = ranked_match_list[1]\n",
    "        \n",
    "\n",
    "        # Exact match\n",
    "        if first_match == expected:\n",
    "            exact_matches += 1\n",
    "            matched.append((nport_issuer_name, expected, first_match))\n",
    "            \n",
    "        # Partial match (i.e., the expected company appears anywhere else in the ranked list)\n",
    "        elif any(match[0] == expected for match in ranked_match_list[0]):\n",
    "            partial_matches += 1\n",
    "            partial_matched.append((nport_issuer_name, expected, ranked_match_list[0]))\n",
    "            \n",
    "        else:\n",
    "            not_matched.append((nport_issuer_name, expected, ranked_match_list[0]))\n",
    "\n",
    "\n",
    "exact_match_accuracy = exact_matches / total_items if total_items > 0 else 0\n",
    "overall_match_accuracy = (exact_matches + partial_matches) / total_items if total_items > 0 else 0\n",
    "\n",
    "\n",
    "print(f\"Total items: {total_items}\")\n",
    "print(f\"Number of exact matches: {exact_matches}\")\n",
    "print(f\"Number of partial matches: {partial_matches}\")\n",
    "print(f\"Exact Match Accuracy: {exact_match_accuracy:.2%}\")\n",
    "print(f\"Overall Accuracy (including partial matches): {overall_match_accuracy:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial matched items:\n",
      "('Tri Alpha Energy', 'tae-technologies', [('xai', 33, 6), ('xai', 33, 7), ('xai', 33, 8), ('tae-technologies', 32, 0), ('tae-technologies', 32, 1)])\n",
      "('Tri Alpha', 'tae-technologies', [('xai', 33, 6), ('xai', 33, 7), ('xai', 33, 8), ('tae-technologies', 32, 0), ('tae-technologies', 32, 1)])\n",
      "('TAE Life Sciences', 'tae-technologies', [('orionis-biosciences', 56, 11), ('orionis-biosciences', 56, 12), ('orionis-biosciences', 56, 13), ('tae-technologies', 55, 0), ('tae-technologies', 55, 1)])\n",
      "('X.AI Corp', 'xai', [('yixia-com', 44, 14), ('xaira-therapeutics', 37, 9), ('xaira-therapeutics', 37, 10), ('xai', 33, 6), ('xai', 33, 7)])\n",
      "('Orion Medicines', 'xaira-therapeutics', [('orionis-biosciences', 71, 11), ('orionis-biosciences', 71, 12), ('orionis-biosciences', 71, 13), ('xaira-therapeutics', 36, 9), ('xaira-therapeutics', 36, 10)])\n"
     ]
    }
   ],
   "source": [
    "print(\"Partial matched items:\")\n",
    "for item in partial_matched:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter duplicates while keeping the highest score for each unique name\n",
    "def filter_unique_matches(ranked_matches):\n",
    "    unique_ranked_matches = {}\n",
    "    for match in ranked_matches:\n",
    "        if match[0] not in unique_ranked_matches or unique_ranked_matches[match[0]][1] < match[1]:\n",
    "            unique_ranked_matches[match[0]] = match\n",
    "    return list(unique_ranked_matches.values())\n",
    "\n",
    "# Apply the filtering to create a new column with filtered matches in partial_matched\n",
    "filtered_partial_matches = [\n",
    "    (nport_issuer_name, expected, ranked_list, filter_unique_matches(ranked_list))\n",
    "    for nport_issuer_name, expected, ranked_list in partial_matched\n",
    "]\n",
    "\n",
    "# Create DataFrame with both original and filtered matches\n",
    "partial_matched_df = pd.DataFrame(filtered_partial_matches, columns=['nport_issuer_name', 'slug', 'rankings', 'filtered_rankings'])\n",
    "partial_matched_df['method'] = 'fuzzy_matched'\n",
    "partial_matched_df.to_csv('data/new_test_data_partial_matched_fuzzy.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of not matches: 1\n",
      "Not matched items:\n",
      "('Grok', 'xai', [('orionis-biosciences', 17, 11), ('orionis-biosciences', 17, 12), ('orionis-biosciences', 17, 13), ('yixia-com', 15, 14), ('tae-technologies', 10, 0)])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of not matches: {len(not_matched)}\")\n",
    "print(\"Not matched items:\")\n",
    "for item in not_matched:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Grok',\n",
       "  'xai',\n",
       "  [('orionis-biosciences', 17, 11),\n",
       "   ('orionis-biosciences', 17, 12),\n",
       "   ('orionis-biosciences', 17, 13),\n",
       "   ('yixia-com', 15, 14),\n",
       "   ('tae-technologies', 10, 0)])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_matched_slugs = [t for t in not_matched] \n",
    "not_matched_slugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match with weights on first word\n",
    "\n",
    "def find_matches_with_weights(phrase, word_list, exact_match_weight=3, first_word_weight=2, other_word_weight=1):\n",
    "    cleaned_words = clean_text(phrase).split()\n",
    "    match_scores = {w: 0 for w in word_list}\n",
    "    \n",
    "    # Handle the first word with a higher weight\n",
    "    if cleaned_words:\n",
    "        first_word = cleaned_words[0]\n",
    "        first_word_regex = re.escape(first_word)\n",
    "        # print(first_word_regex)\n",
    "        compiled_first_word_pattern = re.compile(first_word_regex, re.IGNORECASE)\n",
    "        # print(compiled_first_word_pattern)\n",
    "        \n",
    "        # Add higher weight for first word matches\n",
    "        for list_item in word_list:\n",
    "            if compiled_first_word_pattern.search(clean_text(list_item)):\n",
    "                match_scores[list_item] += first_word_weight\n",
    "    \n",
    "    # Handle other words with a standard weight\n",
    "    for word in cleaned_words[1:]:  \n",
    "        regex_pattern = re.escape(word)\n",
    "        # print(regex_pattern)\n",
    "        compiled_pattern = re.compile(regex_pattern, re.IGNORECASE)       \n",
    "        # print(compiled_pattern)\n",
    "        \n",
    "        for list_item in word_list:\n",
    "            if compiled_pattern.search(clean_text(list_item)):\n",
    "                match_scores[list_item] += other_word_weight\n",
    "\n",
    "    # Add exact match weight if the whole phrase is found\n",
    "    for list_item in word_list:\n",
    "        if re.search(re.escape(clean_text(phrase)), clean_text(list_item)):\n",
    "            match_scores[list_item] += exact_match_weight\n",
    "    \n",
    "    # Rank the results based on the number of matches\n",
    "    ranked_results = sorted(match_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return ranked_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Match Accuracy: 0.00\n",
      "Overall Match Accuracy (including partial matches): 0.00\n",
      "Unmatched items: 1\n"
     ]
    }
   ],
   "source": [
    "correct_first_match = 0\n",
    "partial_matches = 0\n",
    "total_items = len(not_matched_slugs)\n",
    "\n",
    "unmatched_items = []\n",
    "partial_matched_items = []\n",
    "df_rows = []\n",
    "\n",
    "for t in not_matched_slugs:\n",
    "    not_matched_slug = t[1]  # slug\n",
    "    not_matched_nport_issuer_name = t[0]  # nport_issuer_name\n",
    "    \n",
    "    # Get ranked matches\n",
    "    matches = find_matches_with_weights(not_matched_nport_issuer_name, word_list)\n",
    "    \n",
    "    ranked_matches = [match for match in matches if match[1] > 0]\n",
    "    ranked_matches_str = ', '.join([f\"('{item[0]}', {item[1]})\" for item in ranked_matches])\n",
    "    \n",
    "    match_status = \"\"\n",
    "    \n",
    "    if ranked_matches:\n",
    "        # Check the first match\n",
    "        first_match = ranked_matches[0][0]\n",
    "        \n",
    "        if first_match == not_matched_slug:\n",
    "            # Exact match in the first position\n",
    "            correct_first_match += 1\n",
    "            match_status = \"Matched\"\n",
    "        else:\n",
    "            # If second itme exists in the rest of the ranked matches\n",
    "            if any(match[0] == not_matched_slug for match in ranked_matches[1:]):\n",
    "                partial_matches += 1\n",
    "                partial_matched_items.append(t)\n",
    "                match_status = \"Partial Match\"\n",
    "            else:\n",
    "                unmatched_items.append(t)\n",
    "                match_status = \"Not Matched\"\n",
    "    else:\n",
    "        unmatched_items.append(t)\n",
    "        match_status = \"Not Ranked - Not Matched\"\n",
    "\n",
    "    df_rows.append([not_matched_slug, not_matched_nport_issuer_name, ranked_matches_str, match_status])\n",
    "\n",
    "_df = pd.DataFrame(df_rows, columns=['not_matched_slug', 'not_matched_nport_issuer_name', 'Ranked Matches', 'Match Status'])\n",
    "\n",
    "_df.to_csv('data/new_test_data_output.csv', index=False)\n",
    "\n",
    "first_match_accuracy = correct_first_match / total_items if total_items > 0 else 0\n",
    "overall_match_accuracy = (correct_first_match + partial_matches) / total_items if total_items > 0 else 0\n",
    "\n",
    "print(f\"First Match Accuracy: {first_match_accuracy:.2f}\")\n",
    "print(f\"Overall Match Accuracy (including partial matches): {overall_match_accuracy:.2f}\")\n",
    "print(\"Unmatched items:\", len(unmatched_items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_matched_items_df = pd.DataFrame(partial_matched_items, columns=['nport_issuer_name', 'slug', 'rankings'])\n",
    "partial_matched_items_df['method'] = 'perfect_matched'\n",
    "\n",
    "partial_matched_items_df.to_csv('data/new_test_data_partial_matched_perfect.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  slug nport_issuer_name\n",
      "0  xai              Grok\n"
     ]
    }
   ],
   "source": [
    "# create a pd dataframe of the unmatched items\n",
    "not_matched_list = []\n",
    "for t in unmatched_items:\n",
    "    not_matched_list.append([t[1], t[0]])\n",
    "\n",
    "df_not_matched = pd.DataFrame(not_matched_list, columns=['slug', 'nport_issuer_name'])\n",
    "print(df_not_matched.head())\n",
    "df_not_matched.to_csv('data/new_test_data_unmatched_perfect.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  slug nport_issuer_name NAME LEGALENTITYNAME SEARCHALIASES  \\\n",
      "0  xai              Grok  xAI        xAI Corp            []   \n",
      "\n",
      "                                         DESCRIPTION STRUCTURED_DESCRIPTION  \\\n",
      "0  xAI is an AI research company which offers AI ...                          \n",
      "\n",
      "  LIFECYCLESTATUS BANNERMESSAGE         SUB_SECTOR  ...  \\\n",
      "0             NaN           NaN  Data Intelligence  ...   \n",
      "\n",
      "  FORGE_PRICE_ISSUER_TIER FORGE_PRICE FORGE_IMPLIED_VALUATION  \\\n",
      "0                  TIER_1       11.97            2.400000e+10   \n",
      "\n",
      "   FORGE_PRICE_SOURCE_EXTERNAL HAS_IOIS            ARRAY_AGG(FUNDING_DATE)  \\\n",
      "0                         VWAP    False  [\\n  \"2024-05-26 00:00:00.000\"\\n]   \n",
      "\n",
      "  ARRAY_AGG(FR.SHARE_TYPE) NUM_TRADES LAST_CLOSED_TRADE_DATE  \\\n",
      "0       [\\n  \"Series B\"\\n]       28.0                6/27/24   \n",
      "\n",
      "   LAST_PENDING_TRADE_DATE  \n",
      "0                  6/26/24  \n",
      "\n",
      "[1 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "df_forge_price = pd.read_csv('data/forge_price_sample.csv')\n",
    "df_funding_rounds = pd.read_csv('data/funding-rounds.csv')\n",
    "df_issuers = pd.read_csv('data/issuers.csv')\n",
    "df_trade_facts = pd.read_csv('data/trade_facts.csv')\n",
    "\n",
    "\n",
    "df_merged_result = df_issuers.merge(df_forge_price, on='ISSUER_SLUG', how='left') \\\n",
    "            .merge(df_funding_rounds, on='ISSUER_SLUG', how='left') \\\n",
    "            .merge(df_trade_facts, on='ISSUER_SLUG', how='left')\n",
    "# df_merged_result.to_csv('data/merged_result.csv', index=False)\n",
    "df_merged_result = df_merged_result.rename(columns={'ISSUER_SLUG': 'slug'})\n",
    "\n",
    "# join df_not_matched with df_merged_result on left join where df_not_matched.slug = df_merged_result.ISSUER_SLUG \n",
    "df_joined_unmatched_merged_result = df_not_matched.merge(df_merged_result, how='left')\n",
    "\n",
    "print(df_joined_unmatched_merged_result.head())\n",
    "\n",
    "df_joined_unmatched_merged_result.to_csv('data/new_test_data_joined_unmatched_merged_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight functions\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def weight_domicile_country_code(code):\n",
    "    return 1 if code == 'US' else 0\n",
    "\n",
    "def weight_domicile_state_code(code):\n",
    "    return 1 if code == 'DE' else 0\n",
    "\n",
    "\n",
    "funding_round_ranking = {\n",
    "    'seed': 0,\n",
    "    'series a': 1,\n",
    "    'series b': 2,\n",
    "    'series c': 3,\n",
    "    'series d': 4,\n",
    "    'series e': 5,\n",
    "    'series f': 6,\n",
    "    'series g': 7,\n",
    "    'series h': 8,\n",
    "    'series i': 9\n",
    "}\n",
    "def normalize_and_weight_series(series_types):  \n",
    "    if pd.isna(series_types) or not series_types:  # Handle NaN or empty lists\n",
    "        return 0  # Neutral weight for missing values or empty lists\n",
    "\n",
    "    normalized_weights = []\n",
    "    \n",
    "    for series in series_types:\n",
    "        series_lower = series.lower().strip()\n",
    "        \n",
    "        match = re.search(r'series [a-z]', series_lower)\n",
    "        if match:\n",
    "            normalized_type = match.group()\n",
    "            weight = funding_round_ranking.get(normalized_type, 0)\n",
    "            normalized_weights.append(weight)\n",
    "        else:\n",
    "            # Fallback for unrecognized series types (neutral)\n",
    "            normalized_weights.append(0)  \n",
    "    \n",
    "    return max(normalized_weights)  # Return the highest weight in the list\n",
    "\n",
    "\n",
    "def weight_recency(funding_dates_str):\n",
    "    if pd.isna(funding_dates_str) or not funding_dates_str: # Handle NaN or empty lists\n",
    "        return -99999  # Lowest weight for missing or empty values\n",
    "    \n",
    "    funding_dates_clean = re.findall(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}', funding_dates_str)\n",
    "    funding_dates = [datetime.strptime(date, '%Y-%m-%d %H:%M:%S.%f') for date in funding_dates_clean]\n",
    "    most_recent_date = max(funding_dates)\n",
    "    now = datetime.now()\n",
    "    recency_weight = (now - most_recent_date).days\n",
    "    return -recency_weight  # More recent = higher weight (less days, more weight)\n",
    "\n",
    "\n",
    "def weight_price_source(price_source):\n",
    "    if pd.isna(price_source):  # Handle NaN or missing values\n",
    "        return -0.1  # Assign negative weight to missing values\n",
    "    price_source_lower = price_source.lower().strip()\n",
    "    \n",
    "    if re.search(r'vwap', price_source_lower):\n",
    "        return 1  \n",
    "    elif re.search(r'iois', price_source_lower):\n",
    "        return 0.5  \n",
    "    elif re.search(r'primary', price_source_lower):\n",
    "        return 0.2  \n",
    "    else:\n",
    "        return -0.1  # Negative weight for other or unrecognized types\n",
    "\n",
    "def weight_price_issuer_tier(tier):\n",
    "    if pd.isna(tier):  # Handle NaN or missing values\n",
    "        return 0  # Neutral weight for missing\n",
    "    tier_lower = tier.lower().strip()\n",
    "    if tier_lower == 'tier_1':\n",
    "        return 1  # Highest weight for TIER_1\n",
    "    else:\n",
    "        return 0.5  # Neutral or medium weight for other tiers\n",
    "\n",
    "\n",
    "def weight_price(price):\n",
    "    if pd.isna(price):  # Handle NaN or missing values\n",
    "        return 0  # Neutral weight for missing values\n",
    "    return price  # Higher value means higher weight\n",
    "\n",
    "def weight_implied_valuation(valuation):\n",
    "    if pd.isna(valuation):  # Handle NaN or missing values\n",
    "        return 0  # Neutral weight for missing values\n",
    "    return valuation  # Higher value means higher weight\n",
    "\n",
    "#create weight functions for HAS_IOIS, if true less weight, if false more weight\n",
    "def weight_has_iois(has_iois):\n",
    "    return 0 if has_iois else 1\n",
    "\n",
    "# create weight functions for NUM_TRADES, if more trades, higher weight\n",
    "def weight_num_trades(num_trades):\n",
    "    return num_trades if num_trades else 0\n",
    "\n",
    "# create weight functions for LAST_CLOSED_TRADE_DATE, if more recent, higher weight\n",
    "def weight_last_closed_trade_date(last_closed_trade_date):\n",
    "    if pd.isna(last_closed_trade_date) or not last_closed_trade_date:  # Handle NaN or missing values\n",
    "        return -99999  # Lowest weight for missing values\n",
    "    last_closed_trade_date = datetime.strptime(last_closed_trade_date, '%m/%d/%y')\n",
    "    now = datetime.now()\n",
    "    recency_weight = (now - last_closed_trade_date).days\n",
    "    return -recency_weight  # More recent = higher weight (less days, more weight)\n",
    "\n",
    "# create weight functions for LAST_PENDING_TRADE_DATE, if more recent, higher weight\n",
    "def weight_last_pending_trade_date(last_pending_trade_date):\n",
    "    if pd.isna(last_pending_trade_date) or not last_pending_trade_date:  # Handle NaN or missing values\n",
    "        return -99999  # Lowest weight for missing values\n",
    "    last_pending_trade_date = datetime.strptime(last_pending_trade_date, '%m/%d/%y')\n",
    "    now = datetime.now()\n",
    "    recency_weight = (now - last_pending_trade_date).days\n",
    "    return -recency_weight  # More recent = higher weight (less days, more weight)\n",
    "\n",
    "# output_columns += [\n",
    "    \n",
    "#     'YEARFOUNDED',\n",
    "#     'ARCHIVEDAT',\n",
    "#     'CITY'\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 100.00%\n",
      "Top-5 Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# all-MiniLM-L6-v2\n",
    "\n",
    "df_not_matched_2 = df_joined_unmatched_merged_result.copy()\n",
    "# apply normalize_name(str(text)) to df_not_matched_2\n",
    "df_not_matched_2['cleaned_nport_issuer_name'] = df_not_matched_2.apply(lambda row: normalize_name(row['nport_issuer_name']), axis=1)\n",
    "\n",
    "output_columns = [\n",
    "    'slug',\n",
    "    'NAME',\n",
    "    'LEGALENTITYNAME',\n",
    "    'SEARCHALIASES',\n",
    "    'DESCRIPTION',\n",
    "    'STRUCTURED_DESCRIPTION',\n",
    "    'LIFECYCLESTATUS',\n",
    "    'BANNERMESSAGE',\t\n",
    "    'SUB_SECTOR',\n",
    "    'SECTOR',\n",
    "    'WEBSITE',\n",
    "    'CRUNCHBASEURL'\n",
    "]\n",
    "\n",
    "def combine_output_columns(row):\n",
    "    texts = []\n",
    "    for col in output_columns:\n",
    "        if col in row and pd.notnull(row[col]):\n",
    "            texts.append(clean_text(row[col]))\n",
    "    combined_text = ' '.join(texts)\n",
    "    return combined_text\n",
    "\n",
    "df_not_matched_2['combined_output'] = df_not_matched_2.apply(combine_output_columns, axis=1)\n",
    "\n",
    "words_to_find = df_not_matched_2['cleaned_nport_issuer_name'].tolist()\n",
    "word_list = df_not_matched_2['combined_output'].tolist()\n",
    "\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  \n",
    "\n",
    "embeddings_to_find = model.encode(words_to_find, convert_to_numpy=True, normalize_embeddings=True)\n",
    "embeddings_list = model.encode(word_list, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "\n",
    " \n",
    "# Apply the weight functions to the DataFrame\n",
    "df_not_matched_2['DOMICILECOUNTRYCODE_WEIGHT'] = df_not_matched_2['DOMICILECOUNTRYCODE'].apply(weight_domicile_country_code)\n",
    "df_not_matched_2['DOMICILESTATECODE_WEIGHT'] = df_not_matched_2['DOMICILESTATECODE'].apply(weight_domicile_state_code)\n",
    "df_not_matched_2['SHARE_TYPE_WEIGHT'] = df_not_matched_2['ARRAY_AGG(FR.SHARE_TYPE)'].apply(normalize_and_weight_series)\n",
    "df_not_matched_2['FUNDING_DATE_WEIGHT'] = df_not_matched_2['ARRAY_AGG(FUNDING_DATE)'].apply(weight_recency)\n",
    "df_not_matched_2['FORGE_PRICE_SOURCE_WEIGHT'] = df_not_matched_2['FORGE_PRICE_SOURCE_EXTERNAL'].apply(weight_price_source)\n",
    "df_not_matched_2['FORGE_PRICE_ISSUER_TIER_WEIGHT'] = df_not_matched_2['FORGE_PRICE_ISSUER_TIER'].apply(weight_price_issuer_tier)\n",
    "df_not_matched_2['FORGE_PRICE_WEIGHT'] = df_not_matched_2['FORGE_PRICE'].apply(weight_price)\n",
    "df_not_matched_2['FORGE_IMPLIED_VALUATION_WEIGHT'] = df_not_matched_2['FORGE_IMPLIED_VALUATION'].apply(weight_implied_valuation)\n",
    "\n",
    "df_not_matched_2['HAS_IOIS_WEIGHT'] = df_not_matched_2['HAS_IOIS'].apply(weight_has_iois)\n",
    "df_not_matched_2['NUM_TRADES_WEIGHT'] = df_not_matched_2['NUM_TRADES'].apply(weight_num_trades)\n",
    "df_not_matched_2['LAST_CLOSED_TRADE_DATE_WEIGHT'] = df_not_matched_2['LAST_CLOSED_TRADE_DATE'].apply(weight_last_closed_trade_date)\n",
    "df_not_matched_2['LAST_PENDING_TRADE_DATE_WEIGHT'] = df_not_matched_2['LAST_PENDING_TRADE_DATE'].apply(weight_last_pending_trade_date)\n",
    "\n",
    "\n",
    "# Select and normalize the weighted features\n",
    "weighted_feature_columns = [\n",
    "    'DOMICILECOUNTRYCODE_WEIGHT',\n",
    "    'DOMICILESTATECODE_WEIGHT',\n",
    "    'SHARE_TYPE_WEIGHT',\n",
    "    'FUNDING_DATE_WEIGHT',\n",
    "    'FORGE_PRICE_SOURCE_WEIGHT',\n",
    "    'FORGE_PRICE_ISSUER_TIER_WEIGHT',\n",
    "    'FORGE_PRICE_WEIGHT',\n",
    "    'FORGE_IMPLIED_VALUATION_WEIGHT',\n",
    "    'HAS_IOIS_WEIGHT',\n",
    "    'NUM_TRADES_WEIGHT',\n",
    "    'LAST_CLOSED_TRADE_DATE_WEIGHT',\n",
    "    'LAST_PENDING_TRADE_DATE_WEIGHT'\n",
    "]\n",
    "\n",
    "# Fill NaN values with zeros\n",
    "df_not_matched_2[weighted_feature_columns] = df_not_matched_2[weighted_feature_columns].fillna(0.0)\n",
    "\n",
    "# Normalize the weighted features\n",
    "scaler = MinMaxScaler()\n",
    "weighted_features = scaler.fit_transform(df_not_matched_2[weighted_feature_columns])\n",
    "\n",
    "\n",
    "combined_embeddings_to_find = np.hstack((embeddings_to_find, weighted_features))\n",
    "combined_embeddings_list = np.hstack((embeddings_list, weighted_features))\n",
    "cosine_sim_matrix = cosine_similarity(combined_embeddings_to_find, combined_embeddings_list)\n",
    "\n",
    "\n",
    "correct_top1 = 0  # Correct matches at rank 1\n",
    "correct_topk = 0  # Correct matches within top_k\n",
    "top_k = 5 \n",
    "not_matched_results = []\n",
    "partial_matched_results = []\n",
    "\n",
    "for idx, (word, scores) in enumerate(zip(words_to_find, cosine_sim_matrix)):\n",
    "    top_indices = scores.argsort()[-top_k:][::-1]  # Indices of top_k scores in descending order\n",
    "    matches = []\n",
    "    ground_truth_found = False\n",
    "\n",
    "    current_slug = df_not_matched_2.iloc[idx]['slug']\n",
    "    current_npot_issuer_name = df_not_matched_2.iloc[idx]['nport_issuer_name']\n",
    "\n",
    "    for rank, index in enumerate(top_indices):\n",
    "        matched_word = word_list[index]\n",
    "        match_score = scores[index]\n",
    "        matches.append({\n",
    "            'rank': rank + 1,\n",
    "            'match_word': matched_word,\n",
    "            'score': match_score\n",
    "        })\n",
    "\n",
    "        if index == idx:\n",
    "            ground_truth_found = True\n",
    "            if rank == 0:\n",
    "                correct_top1 += 1  # Ground truth is the top match\n",
    "            correct_topk += 1     # Ground truth is within top_k matches\n",
    "            partial_matched_results.append({\n",
    "                    'nport_issuer_name': current_npot_issuer_name, \n",
    "                    'slug': current_slug,\n",
    "                    'rankings': matches\n",
    "                    })\n",
    "\n",
    "    if not ground_truth_found:\n",
    "        not_matched_results.append({\n",
    "                    'slug': current_slug,\n",
    "                    'nport_issuer_name': current_npot_issuer_name, \n",
    "                    'cleaned_nport_issuer_name': word,\n",
    "                    'matches': matches\n",
    "                    })\n",
    "\n",
    "\n",
    "\n",
    "total = len(words_to_find)\n",
    "top1_accuracy = correct_top1 / total\n",
    "topk_accuracy = correct_topk / total\n",
    "\n",
    "print(f\"Top-1 Accuracy: {top1_accuracy:.2%}\")\n",
    "print(f\"Top-{top_k} Accuracy: {topk_accuracy:.2%}\")\n",
    "\n",
    "# save not_matched_results to csv\n",
    "df_not_matched_results = pd.DataFrame(not_matched_results)\n",
    "df_not_matched_results.to_csv('data/new_test_data_not_matched_results.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_matched_results_df = pd.DataFrame(partial_matched_results)\n",
    "partial_matched_items_df['method'] = 'embedding_matched'\n",
    "\n",
    "partial_matched_items_df.to_csv('data/new_test_data_partial_matched_embedding.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overal = 15\n",
    "- matched = 10 - (66%)\n",
    "- partial matched = 5 (33%)\n",
    "- unmatched = 0 - (0%)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
